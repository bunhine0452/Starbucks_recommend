{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import HTMLResponse, FileResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.requests import Request\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# Jupyter Notebook에서 이벤트 루프를 여러 번 실행할 수 있도록 설정\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# FastAPI 애플리케이션 초기화\n",
    "app = FastAPI()\n",
    "\n",
    "# Jinja2 템플릿 설정\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "# 불용어 목록 생성 함수\n",
    "def generate_stopwords(nouns):\n",
    "    noun_counts = Counter(nouns.split())\n",
    "    total_nouns = len(noun_counts)\n",
    "    top_1_percent = int(total_nouns * 0.20)\n",
    "    bottom_1_percent = int(total_nouns * 0.20)\n",
    "    \n",
    "    stopwords = [noun for noun, count in noun_counts.most_common(top_1_percent)]\n",
    "    stopwords += [noun for noun, count in noun_counts.most_common()[:-bottom_1_percent-1:-1]]\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 제거된 명사 추출 함수\n",
    "def filter_nouns(nouns, stopwords):\n",
    "    return ' '.join([noun for noun in nouns.split() if noun not in stopwords])\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/extract-nouns/\", response_class=HTMLResponse)\n",
    "async def extract_nouns_endpoint(request: Request, file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    data = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "    data['nouns'] = data['Content'].apply(extract_nouns)\n",
    "    result_data = data[['Store_Name', 'nouns']]\n",
    "    \n",
    "    # 결과 파일 저장\n",
    "    output_file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "    result_data.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    # nouns 축소 출력 및 '더보기' 기능\n",
    "    result_data['nouns_short'] = result_data['nouns'].apply(lambda x: x[:100] + '...' if len(x) > 100 else x)\n",
    "    \n",
    "    result_html = result_data[['Store_Name', 'nouns_short']].to_html(escape=False, index=False)\n",
    "    code_html = '''\n",
    "    <pre>\n",
    "    {code}\n",
    "    </pre>\n",
    "    '''.format(code='''\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './csv/스타벅스블로그본문.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.head(2)\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "# `Content` 컬럼에서 명사 추출\n",
    "data['nouns'] = data['Content'].apply(extract_nouns)\n",
    "\n",
    "# 결과 저장\n",
    "output_file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "    '''.strip())\n",
    "    \n",
    "    return templates.TemplateResponse(\"result.html\", {\"request\": request, \"result\": result_html, \"code\": code_html, \"file_path\": output_file_path})\n",
    "\n",
    "@app.post(\"/generate-stopwords/\", response_class=HTMLResponse)\n",
    "async def generate_stopwords_endpoint(request: Request, file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    data = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "    \n",
    "    all_nouns = ' '.join(data['nouns'])\n",
    "    stopwords = generate_stopwords(all_nouns)\n",
    "    data['filtered_nouns'] = data['nouns'].apply(lambda x: filter_nouns(x, stopwords))\n",
    "    result_data = data[['Store_Name', 'filtered_nouns']]\n",
    "    \n",
    "    # 결과 파일 저장\n",
    "    output_file_path = './csv/스타벅스키워드추천_결과테스트.csv'\n",
    "    result_data.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    # filtered_nouns 축소 출력 및 '더보기' 기능\n",
    "    result_data['filtered_nouns_short'] = result_data['filtered_nouns'].apply(lambda x: x[:100] + '...' if len(x) > 100 else x)\n",
    "    \n",
    "    result_html = result_data[['Store_Name', 'filtered_nouns_short']].to_html(escape=False, index=False)\n",
    "    code_html = '''\n",
    "    <pre>\n",
    "    {code}\n",
    "    </pre>\n",
    "    '''.format(code='''\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return nouns\n",
    "\n",
    "# 불용어 목록 생성 함수\n",
    "def generate_stopwords(nouns):\n",
    "    noun_counts = Counter(nouns)\n",
    "    total_nouns = len(noun_counts)\n",
    "    top_1_percent = int(total_nouns * 0.01)\n",
    "    bottom_1_percent = int(total_nouns * 0.01)\n",
    "    \n",
    "    stopwords = [noun for noun, count in noun_counts.most_common(top_1_percent)]\n",
    "    stopwords += [noun for noun, count in noun_counts.most_common()[:-bottom_1_percent-1:-1]]\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 제거된 명사 추출 함수\n",
    "def filter_nouns(nouns, stopwords):\n",
    "    return ' '.join([noun for noun in nouns if noun not in stopwords])\n",
    "\n",
    "data['filtered_nouns'] = data['nouns'].apply(lambda x: filter_nouns(x, stopwords))\n",
    "\n",
    "# 결과 저장\n",
    "output_file_path = './csv/스타벅스키워드추천_결과테스트.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "    '''.strip())\n",
    "    \n",
    "    return templates.TemplateResponse(\"result.html\", {\"request\": request, \"result\": result_html, \"code\": code_html, \"file_path\": output_file_path})\n",
    "\n",
    "@app.get(\"/download/\")\n",
    "async def download_file(file_path: str):\n",
    "    return FileResponse(file_path, media_type='application/octet-stream', filename=os.path.basename(file_path))\n",
    "\n",
    "# FastAPI 서버 실행\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
