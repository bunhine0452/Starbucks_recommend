{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 모듈을 pd라는 별칭으로 임포트\n",
    "import pandas as pd\n",
    "\n",
    "# konlpy 패키지에서 Okt 형태소 분석기를 임포트\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './data/필터링된_스타벅스블로그본문.csv'  # 데이터가 저장된 CSV 파일 경로\n",
    "data = pd.read_csv(file_path)  # CSV 파일을 읽어서 DataFrame으로 저장\n",
    "# data = data.head(2)  # 데이터의 첫 2개 행만 선택\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()  # Okt 객체를 생성하여 형태소 분석기 초기화\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    \"\"\"\n",
    "    입력된 텍스트에서 명사만 추출하는 함수\n",
    "    :param text: 형태소 분석을 할 텍스트\n",
    "    :return: 명사만 공백으로 구분하여 반환한 문자열\n",
    "    \"\"\"\n",
    "    tokens = okt.pos(text)  # 텍스트를 형태소 단위로 토큰화하고 각 토큰의 품사를 태깅\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]  # 태깅된 토큰 중 명사(Noun)만 추출\n",
    "    return ' '.join(nouns)  # 추출된 명사를 공백으로 구분하여 하나의 문자열로 반환\n",
    "\n",
    "# `Content` 컬럼에서 명사 추출\n",
    "data['nouns'] = data['Content'].apply(extract_nouns)  # DataFrame의 'Content' 열에 대해 명사 추출 함수를 적용하여 새로운 'nouns' 열 생성\n",
    "\n",
    "# 결과 저장\n",
    "output_file_path = './data/스타벅스명사추출다시.csv'  # 결과를 저장할 CSV 파일 경로\n",
    "data.to_csv(output_file_path, index=False)  # DataFrame을 CSV 파일로 저장, 인덱스는 저장하지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Name</th>\n",
       "      <th>nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스타벅스 을지로4가역점</td>\n",
       "      <td>커피 디저트 여행 을지로 역 카페 스타벅스 을지로 역점 돈 산 후기 세상 여행 블로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스타벅스 을지로경기빌딩점</td>\n",
       "      <td>냠냠냠 을지로 대형 카페 스타벅스 을지로 경기 빌딩 점 녜 스타벅스 이용 저 스타벅...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store_Name                                              nouns\n",
       "0   스타벅스 을지로4가역점  커피 디저트 여행 을지로 역 카페 스타벅스 을지로 역점 돈 산 후기 세상 여행 블로...\n",
       "1  스타벅스 을지로경기빌딩점  냠냠냠 을지로 대형 카페 스타벅스 을지로 경기 빌딩 점 녜 스타벅스 이용 저 스타벅..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "data.drop(columns='Content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenization_morp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, BertConfig\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenization_morp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MecabTokenizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./csv/스타벅스블로그본문.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenization_morp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from tokenization_morp import MecabTokenizer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = './csv/스타벅스블로그본문.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.head(2)\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# 전처리 작업\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 전화번호 제거\n",
    "    text = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', '', text)\n",
    "    # 날짜 제거 (예: 2019.4.17 15:42)\n",
    "    text = re.sub(r'\\d{4}\\.\\d{1,2}\\.\\d{1,2} \\d{1,2}:\\d{2}', '', text)\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r'[!?,]', '', text)\n",
    "    # 모든 이모티콘 제거\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "    # 기타 특수문자 제거\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_content'] = df['Content'].apply(clean_text)\n",
    "\n",
    "# 형태소 분석기 설정\n",
    "mecab_dic_path = '/Users/hyunbin/mecab-ko-dic-2.1.1-20180720'\n",
    "mecab_tokenizer = MecabTokenizer(mecab_dic_path)\n",
    "\n",
    "# KorBERT 모델 로드\n",
    "config_path = 'bert_config.json'\n",
    "model_bin_path = 'trained_bert_model.bin'  # 학습된 모델 파일 경로\n",
    "vocab_file_path = 'vocab.korean_morp.list'\n",
    "\n",
    "config = BertConfig.from_json_file(config_path)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(None, config=config, state_dict=torch.load(model_bin_path))\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file_path, do_lower_case=False)\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(texts, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_dict = mecab_tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks\n",
    "\n",
    "texts = df['cleaned_content'].tolist()\n",
    "input_ids, attention_masks = prepare_dataset(texts)\n",
    "\n",
    "# 키워드 추출\n",
    "def extract_keywords(input_ids, attention_masks, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(input_ids)):\n",
    "            outputs = model(input_ids[i].unsqueeze(0), token_type_ids=None, attention_mask=attention_masks[i].unsqueeze(0))\n",
    "            logits = outputs.logits\n",
    "            prediction = torch.argmax(logits, dim=1).item()\n",
    "            predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "predictions = extract_keywords(input_ids, attention_masks, bert_model)\n",
    "\n",
    "# 키워드 결과를 DataFrame에 추가\n",
    "df['keywords'] = predictions\n",
    "\n",
    "# 매장명과 키워드만 추출\n",
    "result_df = df[['Store_Name', 'keywords']]\n",
    "\n",
    "# 결과 저장\n",
    "result_df.to_csv('./csv/스타벅스명사추출테스트.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Name</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스타벅스 을지로4가역점</td>\n",
       "      <td>가끔 가득 계란 공휴일 기본 때문 라이트 로비 매우 보고 보기 보늬밤 보시 보이시 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스타벅스 을지로경기빌딩점</td>\n",
       "      <td>가끔 거기 거리 걸음 결제 경영 고통 곳도 구경 구역 굿즈들 그거 근처 기기 네이버...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>스타벅스 을지로국제빌딩점</td>\n",
       "      <td>가게 계속 고요 구역 노트북 리뷰 마감 마카롱 물가 뭔가 바로 별로 보고 분위기 블...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스타벅스 을지로삼화타워점</td>\n",
       "      <td>구경 그냥 근처 글레이 남대문로 녹차 느낌 는걸 대기업 대신 대한민국 던데 동시 먹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스타벅스 을지로한국빌딩점</td>\n",
       "      <td>가게 가기 가야 가지 검사 고요 과정 나이 네이버 노트북 달라 대해 대형 디저트 라...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store_Name                                           Keywords\n",
       "0   스타벅스 을지로4가역점  가끔 가득 계란 공휴일 기본 때문 라이트 로비 매우 보고 보기 보늬밤 보시 보이시 ...\n",
       "1  스타벅스 을지로경기빌딩점  가끔 거기 거리 걸음 결제 경영 고통 곳도 구경 구역 굿즈들 그거 근처 기기 네이버...\n",
       "2  스타벅스 을지로국제빌딩점  가게 계속 고요 구역 노트북 리뷰 마감 마카롱 물가 뭔가 바로 별로 보고 분위기 블...\n",
       "3  스타벅스 을지로삼화타워점  구경 그냥 근처 글레이 남대문로 녹차 느낌 는걸 대기업 대신 대한민국 던데 동시 먹...\n",
       "4  스타벅스 을지로한국빌딩점  가게 가기 가야 가지 검사 고요 과정 나이 네이버 노트북 달라 대해 대형 디저트 라..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 모듈을 pd라는 별칭으로 임포트\n",
    "import pandas as pd\n",
    "\n",
    "# collections 모듈에서 Counter 클래스를 임포트\n",
    "from collections import Counter\n",
    "\n",
    "# scikit-learn 패키지에서 TfidfVectorizer를 임포트\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# konlpy 패키지에서 Okt 형태소 분석기를 임포트\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './data/스타벅스명사추출다시.csv'  # 데이터가 저장된 CSV 파일 경로\n",
    "data = pd.read_csv(file_path)  # CSV 파일을 읽어서 DataFrame으로 저장\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()  # Okt 객체를 생성하여 형태소 분석기 초기화\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    \"\"\"\n",
    "    입력된 텍스트에서 명사만 추출하는 함수\n",
    "    :param text: 형태소 분석을 할 텍스트\n",
    "    :return: 명사 리스트\n",
    "    \"\"\"\n",
    "    tokens = okt.pos(text)  # 텍스트를 형태소 단위로 토큰화하고 각 토큰의 품사를 태깅\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]  # 태깅된 토큰 중 명사(Noun)만 추출\n",
    "    return nouns  # 명사 리스트 반환\n",
    "\n",
    "# 불용어 목록 생성 함수\n",
    "def generate_stopwords(nouns):\n",
    "    \"\"\"\n",
    "    명사 리스트에서 빈도 상위 및 하위 1%의 단어를 불용어로 지정하는 함수\n",
    "    :param nouns: 명사 리스트\n",
    "    :return: 불용어 리스트\n",
    "    \"\"\"\n",
    "    noun_counts = Counter(nouns)  # 명사 리스트에서 각 명사의 빈도수 계산\n",
    "    total_nouns = len(noun_counts)  # 명사의 총 종류 수 계산\n",
    "    top_1_percent = int(total_nouns * 0.20)  # 상위 1%에 해당하는 명사 수 계산\n",
    "    bottom_1_percent = int(total_nouns * 0.20)  # 하위 1%에 해당하는 명사 수 계산\n",
    "    \n",
    "    # 상위 1% 빈도 명사와 하위 1% 빈도 명사를 불용어로 지정\n",
    "    stopwords = [noun for noun, count in noun_counts.most_common(top_1_percent)]\n",
    "    stopwords += [noun for noun, count in noun_counts.most_common()[:-bottom_1_percent-1:-1]]\n",
    "    return stopwords  # 불용어 리스트 반환\n",
    "\n",
    "# 불용어 제거된 명사 추출 함수\n",
    "def filter_nouns(nouns, stopwords):\n",
    "    \"\"\"\n",
    "    명사 리스트에서 불용어를 제거하는 함수\n",
    "    :param nouns: 명사 리스트\n",
    "    :param stopwords: 불용어 리스트\n",
    "    :return: 불용어가 제거된 명사 문자열\n",
    "    \"\"\"\n",
    "    return ' '.join([noun for noun in nouns if noun not in stopwords])  # 불용어를 제거한 명사들을 공백으로 구분하여 하나의 문자열로 반환\n",
    "\n",
    "# 각 본문에 대해 독립적으로 명사 추출, 불용어 제거, TF-IDF 적용\n",
    "store_names = []  # 매장 이름을 저장할 리스트 초기화\n",
    "keywords = []  # 키워드를 저장할 리스트 초기화\n",
    "\n",
    "# 각 행(블로그 글)에 대해 반복\n",
    "for index, row in data.iterrows():\n",
    "    content = row['Content']  # 블로그 글 내용\n",
    "    store_name = row['Store_Name']  # 매장 이름\n",
    "    \n",
    "    # 명사 추출\n",
    "    nouns = extract_nouns(content)\n",
    "    \n",
    "    # 불용어 목록 생성\n",
    "    stopwords = generate_stopwords(nouns)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    filtered_nouns = filter_nouns(nouns, stopwords)\n",
    "    \n",
    "    # TF-IDF 적용\n",
    "    vectorizer = TfidfVectorizer(max_features=100)  # 매장별로 상위 100개의 키워드 추출\n",
    "    X = vectorizer.fit_transform([filtered_nouns])  # TF-IDF 행렬 생성\n",
    "    \n",
    "    # 키워드 추출\n",
    "    tfidf_keywords = vectorizer.get_feature_names_out()  # TF-IDF 벡터에서 키워드 추출\n",
    "    \n",
    "    # 저장\n",
    "    store_names.append(store_name)  # 매장 이름 리스트에 추가\n",
    "    keywords.append(' '.join(tfidf_keywords))  # 키워드 리스트에 추가\n",
    "\n",
    "# 결과 저장\n",
    "result_df = pd.DataFrame({'Store_Name': store_names, 'Keywords': keywords})  # 매장 이름과 키워드를 DataFrame으로 저장\n",
    "output_file_path = './csv/스타벅스키워드추천_결과테스트.csv'  # 결과를 저장할 CSV 파일 경로\n",
    "result_df.to_csv(output_file_path, index=False)  # DataFrame을 CSV 파일로 저장, 인덱스는 저장하지 않음\n",
    "\n",
    "# 결과 확인\n",
    "result_df.head()  # 결과 DataFrame의 첫 5개 행 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Name</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스타벅스 을지로4가역점</td>\n",
       "      <td>라떼 매장 메뉴 스벅 시간 음료 주말 중구 카페 트윈타워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>스타벅스 을지로경기빌딩점</td>\n",
       "      <td>남대문로 매장 서울 시간 을지로입구역 제로 중구 카공 카페 평일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>스타벅스 을지로국제빌딩점</td>\n",
       "      <td>라떼 매장 반납 방문 서울 세척 에코 이용 제로 중구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스타벅스 을지로삼화타워점</td>\n",
       "      <td>말차 사용 서울 시간 유저 을지로입구역 음료 주차 중구 카페</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스타벅스 을지로한국빌딩점</td>\n",
       "      <td>국제 명동 반납 방문 생각 서울 을지로입구역 제로 주문 한국</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>스타벅스 황학사거리점</td>\n",
       "      <td>메뉴 상가 서울 서울특별시 성동구 스벅 시간 주차 카드 커피</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>스타벅스 황학캐슬점</td>\n",
       "      <td>거리 매장 서울특별시 아메리카노 주차 중구 청계천 카페 커피 학사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>스타벅스 회기역사거리점</td>\n",
       "      <td>동대문구 맛집 메뉴 브루 시간 에스프레소 음료 이문로 초콜릿 콜드</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>스타벅스 회현역점</td>\n",
       "      <td>메뉴 사진 서울 아메리카노 주문 중구 카드 크리스마스 크림 퇴계로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>스타벅스 효창공원앞역점</td>\n",
       "      <td>롯데 백범로 선물 쉐이크 아메리카노 영업 음료 제품 치즈 캐슬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Store_Name                              Keywords\n",
       "0      스타벅스 을지로4가역점       라떼 매장 메뉴 스벅 시간 음료 주말 중구 카페 트윈타워\n",
       "1     스타벅스 을지로경기빌딩점   남대문로 매장 서울 시간 을지로입구역 제로 중구 카공 카페 평일\n",
       "2     스타벅스 을지로국제빌딩점         라떼 매장 반납 방문 서울 세척 에코 이용 제로 중구\n",
       "3     스타벅스 을지로삼화타워점     말차 사용 서울 시간 유저 을지로입구역 음료 주차 중구 카페\n",
       "4     스타벅스 을지로한국빌딩점     국제 명동 반납 방문 생각 서울 을지로입구역 제로 주문 한국\n",
       "...             ...                                   ...\n",
       "1919    스타벅스 황학사거리점     메뉴 상가 서울 서울특별시 성동구 스벅 시간 주차 카드 커피\n",
       "1920     스타벅스 황학캐슬점  거리 매장 서울특별시 아메리카노 주차 중구 청계천 카페 커피 학사\n",
       "1921   스타벅스 회기역사거리점  동대문구 맛집 메뉴 브루 시간 에스프레소 음료 이문로 초콜릿 콜드\n",
       "1922      스타벅스 회현역점  메뉴 사진 서울 아메리카노 주문 중구 카드 크리스마스 크림 퇴계로\n",
       "1923   스타벅스 효창공원앞역점    롯데 백범로 선물 쉐이크 아메리카노 영업 음료 제품 치즈 캐슬\n",
       "\n",
       "[1924 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = pd.read_csv('스타벅스_키워드_추천_결과.csv')\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [25443]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52487 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52994 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53008 - \"POST /extract-nouns/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/_dw7rbn14xl_4vkcr1327d040000gn/T/ipykernel_25443/469787622.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_data['nouns_short'] = result_data['nouns'].apply(lambda x: x[:100] + '...' if len(x) > 100 else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53009 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53011 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53011 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53011 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53011 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [25443]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 167>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# FastAPI 서버 실행\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/uvicorn/main.py:577\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    575\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muds \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39muds):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/uvicorn/server.py:65\u001b[0m, in \u001b[0;36mServer.run\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py:83\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     81\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py:119\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 119\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/asyncio/tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py:195\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    193\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/uvicorn/server.py:69\u001b[0m, in \u001b[0;36mServer.serve\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture_signals():\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serve(sockets)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/uvicorn/server.py:328\u001b[0m, in \u001b[0;36mServer.capture_signals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# If we did gracefully shut down due to a signal, try to\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# trigger the expected behaviour now; multiple signals would be\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# done LIFO, see https://stackoverflow.com/questions/48434964\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m captured_signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captured_signals):\n\u001b[0;32m--> 328\u001b[0m     \u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaptured_signal\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import HTMLResponse, FileResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.requests import Request\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Okt\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# Jupyter Notebook에서 이벤트 루프를 여러 번 실행할 수 있도록 설정\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# FastAPI 애플리케이션 초기화\n",
    "app = FastAPI()\n",
    "\n",
    "# Jinja2 템플릿 설정\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "# 불용어 목록 생성 함수\n",
    "def generate_stopwords(nouns):\n",
    "    noun_counts = Counter(nouns.split())\n",
    "    total_nouns = len(noun_counts)\n",
    "    top_1_percent = int(total_nouns * 0.20)\n",
    "    bottom_1_percent = int(total_nouns * 0.20)\n",
    "    \n",
    "    stopwords = [noun for noun, count in noun_counts.most_common(top_1_percent)]\n",
    "    stopwords += [noun for noun, count in noun_counts.most_common()[:-bottom_1_percent-1:-1]]\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 제거된 명사 추출 함수\n",
    "def filter_nouns(nouns, stopwords):\n",
    "    return ' '.join([noun for noun in nouns.split() if noun not in stopwords])\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/extract-nouns/\", response_class=HTMLResponse)\n",
    "async def extract_nouns_endpoint(request: Request, file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    data = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "    data['nouns'] = data['Content'].apply(extract_nouns)\n",
    "    result_data = data[['Store_Name', 'nouns']]\n",
    "    \n",
    "    # 결과 파일 저장\n",
    "    output_file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "    result_data.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    # nouns 축소 출력 및 '더보기' 기능\n",
    "    result_data['nouns_short'] = result_data['nouns'].apply(lambda x: x[:100] + '...' if len(x) > 100 else x)\n",
    "    \n",
    "    result_html = result_data[['Store_Name', 'nouns_short']].to_html(escape=False, index=False)\n",
    "    code_html = '''\n",
    "    <pre>\n",
    "    {code}\n",
    "    </pre>\n",
    "    '''.format(code='''\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './csv/스타벅스블로그본문.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.head(2)\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "# `Content` 컬럼에서 명사 추출\n",
    "data['nouns'] = data['Content'].apply(extract_nouns)\n",
    "\n",
    "# 결과 저장\n",
    "output_file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "    '''.strip())\n",
    "    \n",
    "    return templates.TemplateResponse(\"result.html\", {\"request\": request, \"result\": result_html, \"code\": code_html, \"file_path\": output_file_path})\n",
    "\n",
    "@app.post(\"/generate-stopwords/\", response_class=HTMLResponse)\n",
    "async def generate_stopwords_endpoint(request: Request, file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    data = pd.read_csv(StringIO(content.decode('utf-8')))\n",
    "    \n",
    "    all_nouns = ' '.join(data['nouns'])\n",
    "    stopwords = generate_stopwords(all_nouns)\n",
    "    data['filtered_nouns'] = data['nouns'].apply(lambda x: filter_nouns(x, stopwords))\n",
    "    result_data = data[['Store_Name', 'filtered_nouns']]\n",
    "    \n",
    "    # 결과 파일 저장\n",
    "    output_file_path = './csv/스타벅스키워드추천_결과테스트.csv'\n",
    "    result_data.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    # filtered_nouns 축소 출력 및 '더보기' 기능\n",
    "    result_data['filtered_nouns_short'] = result_data['filtered_nouns'].apply(lambda x: x[:100] + '...' if len(x) > 100 else x)\n",
    "    \n",
    "    result_html = result_data[['Store_Name', 'filtered_nouns_short']].to_html(escape=False, index=False)\n",
    "    code_html = '''\n",
    "    <pre>\n",
    "    {code}\n",
    "    </pre>\n",
    "    '''.format(code='''\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = './csv/스타벅스명사추출결과테스트.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    tokens = okt.pos(text)\n",
    "    nouns = [word for word, pos in tokens if pos in ['Noun']]\n",
    "    return nouns\n",
    "\n",
    "# 불용어 목록 생성 함수\n",
    "def generate_stopwords(nouns):\n",
    "    noun_counts = Counter(nouns)\n",
    "    total_nouns = len(noun_counts)\n",
    "    top_1_percent = int(total_nouns * 0.01)\n",
    "    bottom_1_percent = int(total_nouns * 0.01)\n",
    "    \n",
    "    stopwords = [noun for noun, count in noun_counts.most_common(top_1_percent)]\n",
    "    stopwords += [noun for noun, count in noun_counts.most_common()[:-bottom_1_percent-1:-1]]\n",
    "    return stopwords\n",
    "\n",
    "# 불용어 제거된 명사 추출 함수\n",
    "def filter_nouns(nouns, stopwords):\n",
    "    return ' '.join([noun for noun in nouns if noun not in stopwords])\n",
    "\n",
    "data['filtered_nouns'] = data['nouns'].apply(lambda x: filter_nouns(x, stopwords))\n",
    "\n",
    "# 결과 저장\n",
    "output_file_path = './csv/스타벅스키워드추천_결과테스트.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "    '''.strip())\n",
    "    \n",
    "    return templates.TemplateResponse(\"result.html\", {\"request\": request, \"result\": result_html, \"code\": code_html, \"file_path\": output_file_path})\n",
    "\n",
    "@app.get(\"/download/\")\n",
    "async def download_file(file_path: str):\n",
    "    return FileResponse(file_path, media_type='application/octet-stream', filename=os.path.basename(file_path))\n",
    "\n",
    "# FastAPI 서버 실행\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
